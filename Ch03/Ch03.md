
## Document Modeling

[modeling]: #modeling

Modeling your data is important&mdash;crucially so. That's because RavenDB, and any other database, needs you to understand 
how it's actually storing information, what features it has available for you to utilize, what impact 
different modeling decisions will have on your data, the amount of work the database needs to do, etc.

If you get it wrong, you may be forcing the database to do an exponential amount of additional work. On the other hand, if you 
play to the database's strengths, you can get a robust and easy-to-build system. So, overall, no pressure whatsoever.

Relational-based modeling is so frequently encountered that, in most cases, you don't even see people talk about the relational
part. They just consider that simple "data modeling." But when you try to apply a relational modeling solution to a non-relational system, the end result is usually...suboptimal.

> **Documents aren't flat**
>
> Documents, unlike a row in a relational database, aren't flat. You aren't limited to just storing keys and values. Instead, 
> you can store complex object graphs as a single document. That includes arrays, dictionaries and trees. Unlike a relational
> database, where a row can only contain simple values and more complex data structures need to be stored as relations,
> you donâ€™t need to work hard to map your data into a document database.
>
> That gives you a major advantage. It simplifies a lot of common tasks, as you'll see in this chapter. 

The problem is that this is extremely well-entrenched behavior, to the point where most people don't even realize they're
making decisions based on what would work best for a relational database. So the first section of this chapter is 
going to deal with how to get away from the relational mindset, and the second part will focus on how to model data for RavenDB.

### Beyond relational data modeling

You've likely used a relational database in the past. That means you've learned about normalization and how 
important it is. Words like "data integrity" are thrown around quite often. But the original purpose of normalization 
had everything to do with reducing duplication to the maximum extent.

A common example of normalization is addresses. Instead of storing a customer's address on every order that he has, we can 
simply store the address ID in the order, and we've saved ourselves the need to update the address in multiple locations. 
You can see a sample of such a schema in Figure 3.1.

![A simple relational schema for orders](./Ch03/img01.png)

You've seen (and probably wrote) such schemas before. And at a glance, you'll probably agree that this is a reasonable way to 
structure a database for order management. Now, let's explore what happens when the customer wishes to change his address. The 
way the database is set up, we can just update a single row in the addresses table. Great, we're done.

Except...we aren't. We've just introduced a subtle but deadly data corruption for our database. If that customer had existing orders, 
both those orders and the customer information now point at the same address. Updating the address for the customer 
therefore will also update the address for _all of its orders_. When we look at one of those orders, we won't see the 
address it was shipped to but rather the _current_ customer address.

> **When a data modeling error means calling the police**
> 
> In the real world, I've seen such things happen with payroll systems and paystubs (or "payslips," across the pond). An employee 
> had married and changed her bank account information to a new shared bank account with her husband. The couple also wanted to purchase a 
> home, so they applied for a mortgage. As part of that, they had to submit paystubs from the past several months. The 
> employee asked her HR department to send over her most the recent stubs. When the bank saw paystubs made to an 
> account that didn't exist, they suspected fraud. The mortgage was denied and the police were called. An 
> unpleasant situation all around.^[This ended up being sorted out eventually by uttering the magic words: "computer error." 
> But it was very exciting for a while there.]

It's common for this issue to be blamed on bad modeling decisions (and I agree). The problem is that 
a more appropriate model is complex to build and work with, expensive to query and hard to reason about in 
general. 

A lot of the widespread wisdom about data modeling is limited to only seeing the world through relational eyes. The relation 
model offers us tables, rows and columns to store our data, and it's up to us to hammer the data into the right shape so the 
relational database can accept it. Along the way, we have to deal with an impedance mismatch between how software (and our 
minds) model the data and how we're forced to store it to the relational database.

A document database like RavenDB doesn't solve this problem completely. It's entirely possible to construct models that would 
be a poor fit for the way RavenDB stores data. However, the way most business applications (and in general OLTP systems) 
think about their data is a excellent fit for RavenDB.

You can read more about this by looking at `Domain Driven Design`^[The 
[book]( https://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software/dp/0321125215) is a bit dry, but I remember 
being very impressed when I read it the first time.] and, in particular, about the notion of an `Aggregate Root`.

> **What are aggregates?**
>
> One of the best definitions I have read is [Martin Fowler](https://martinfowler.com/bliki/DDD_Aggregate.html)'s:
>  
>> A DDD aggregate is a cluster of domain objects that can be treated as a single unit. An example may be an order and its 
>> line items. These will be separate objects, but it's useful to treat the order (together with its line items) as a single 
>> aggregate.

>> An aggregate will have one of its component objects be the aggregate root. Any references from outside the aggregate should
>> only go to the aggregate root. The root can thus ensure the integrity of the aggregate as a whole.

In the context of RavenDB, this is highly relevant since every RavenDB document is an aggregate and every aggregate is a 
document. Modeling techniques for aggregates work well for document-oriented design, and that gives you a great resource
for modeling in the real world. 

But before we can start running, we need to learn to walk. So let's start by learning how to use RavenDB with the most basic of 
modeling techniques: none.

### Using RavenDB as a key/value store

RavenDB is a document database, but it's also quite good at being just a key/value store. That's mostly accidental, but as 
part of making sure we have a really fast database, we also significantly reduced the cost of just storing and loading 
documents without any of the other benefits of RavenDB.

With the restriction that the data must be JSON, using RavenDB as a key/value store makes a lot of sense. It also makes 
sense to use RavenDB to cache information, store the shopping cart during purchase process or just hold on to the user session 
data&mdash;all classic models for key/value stores.

RavenDB doesn't impose any additional costs on storing/loading documents by default, so you get to use a fast database 
with the simplest of all access models. Typically, complexity in key/value modeling resides in generating the appropriate key and 
what sort of operations you can do on it.

For the purpose of using RavenDB as a key/value store, it's likely the following features will be most relevant:

* You can generate the document key independently of the collection used.
* Loading saved document(s) can be done in a single call, by providing the ID(s) to load. 
* RavenDB provides automatic expiration for documents, which you can use with caching/session data.
* You can perform searches using the document key as a prefix (and even using glob-like searches).
* Includes can be used to fetch related data without having to make multiple remote calls.

The nice thing about using RavenDB as a key/value store for such information is that you aren't _limited_ to just those 
key/value operations. If you're storing shopping cart data inside RavenDB, you can also start pulling data from there. You can see the  most popular item being purchased or do projections on inventory
or any of a whole host of things that will provide you with useful knowledge.

In a typical key/value store (`Redis`, for example), you'll have to manually track such things. However, RavenDB allows you to 
query and aggregate the data very easily.

But using RavenDB just for key/value data is somewhat of a waste, given what it's capable of doing with your data. So, without
further ado, let's dive into document modeling in RavenDB.

### Modeling considerations in RavenDB

The reason we cover modeling in this chapter is that I want you to get a feel for some of 
RavenDB's capabilities before we start discussing modeling techniques. The kind of features that RavenDB has
to offer are directly relevant to the models you'll write.

> **RavenDB as the application database**
>
> A common bad practice is to integrate between different applications using a shared database. This has been looked down upon 
> for quite a while. RavenDB makes no attempt to cater to those who use it as shared database. Instead, it shines in its role as
> an application database.
>
> Typical shared database solution suffer because they attempt to force disparate applications to treat the data in the same 
> manner. This is a pretty bad idea, since different application tend to have different idea about even very similar concepts.
> The common example being when you have a billing application and a fullfilment application. Both have the concept of a customer
> and both actually refer to the same thing when they talk about a customer. But the fullfilment application need to keep track
> of very different data from the billing application. What will the fullfliment app do with the `CreditScore` field, and what
> can the billing app gain from looking at the `ShippingManifest` details?
>
> Each application should model its data independently, and evolve it as it needs to. Keeping everything in a shared database lead
> to a of required coordination between the various application teams and result in increased complexity in evolving the 
> data model as the various applications are being work on.
> 
> However, even though the different applications are separate, they still need to share data between them. How do you do that
> with RavenDB. We'll talk about this a lot more in the chapter on [Sharing data and making friends with ETL](#integrations), 
> but the answer to that is that each application
> has its own database, and we can setup information flow between the servers by using RavenDB's builtin ETL capabilities. 
> 
> In this way, each application is independent and can evolve and grow on its own, with well defined boundaries and intergation
> points to its siblings.

We typically encourage you to using DDD techniques for modeling in RavenDB, since they fit so well. A core design principle 
for modeling documents is that they should be: Independent, Isolated and Coherent.

* *Independent* - a document should have its own separate existence from any other documents. 
* *Isolated* - a document can change independently from other documents. 
* *Coherent* - a document should be legible on its own, without referencing other documents.

This is actually easier to explain using negation. A document is not independent if it can't stand up on its own. A good 
example of a dependent document would be the `OrderLine`. An `OrderLine` doesn't really have a meaning outside the scope of 
an `Order`, so it should be a `Value Object` inside the `Order` document. An `Entity` is defined as "a thing with distinct and 
independent existence". And that is how you treat documents, as entities. If a document exists only as a part of larger whole, the 
model should be refactor so that single large whole is a single document.

A documnet is not isolated if changing this document also requires updating additional document(s). For example, if you update the 
`CustomerBilling` document and also need to update the `Customer` document, they are not isolated.. A document should be able to 
change independently from other documents, otherwise, there is an unclear transaction boundary involved. With each document isolated, 
the transaction boundary (what can change together) is much clearer, it is drawn at the document level.

A document is not coherent if it is not possible to make sense of the document with just the information on that document. If you need
to go and lookup additional state or information from other documents, that document isn't coherent. A good example would be the 
`Order` document and the `ShippingMethod`. If, in order to ship an order, we need to go and look at the `Customer` document, then the
`Order` document is not coherent. 

#### Looking at physical documents

A really good trick for document modeling is to consider the data as, well... documents. What I mean by that, _physical_ document, 
the kind that you actually hold up in your hand and can get a paper cut from. A lot of the same considerations that apply in the real
world apply for document modeling.

If I hand you a document and tell you that you need to fill up _this_ form, and then go and update _that_ form, you'll right consider 
the process (normal goverment behavior / bureaucratic / kafkaesque)^[Circle the approach choise]. If I gave you a form and told you 
that in order to understand it you had to consult this other document... you get my point.

When modeling, I find that it really helps when I'm picture the document in its printed form. If it make sense as a printed page, it 
is probably valid in term of document modeling.

#### Denormalization and other scary beasts

Anyone taking a relational modeling course had the notion that "Store a Fact Only Once" as  just short of sacred (or maybe not so 
short). The basic idea is that if you store a fact only a single time, you are preventing update anomolies. Such as when you updated 
a person date of birth in the employee record, but forgot to update it on the "send a card" listing. 

I wholeheartedly agree with this statement, in principal. But the problem is that sometimes, this isn't the same fact at all, even if
it looks the same. Let us consider the notion of a `Customer`, `Order` and what the `Address` property means in this case. The 
`Customer` entity has an `Address` property, that holds the address of the customer, and the `Order` which has a `ShipTo` proeprty.

Why are we duplicating this information? The answer is that this isn't the same information, even if the content is the same. On the 
one hand, we have the `Customer.Address` which represent the _current_ customer's address. On the other hand, we have the 
`Order.ShipTo` which represent a _point in time_ copy of the customer address at the time we created the order. Those are very 
important distinctions. 

One of the more common objections to the kind of modeling advice in this chapter is that the data is denormalized. And that is true, 
but for the most part, even if the same data appears in multiple locations, it doesn't have the same semantic meaning. And the notion
of point in time data is quite important in many fields. 

RavenDB has quite a few features that help in working with normalized data (`LoadDocument` and transformers are the two main one, 
covered in [Part III](#indexing).), but you need to consider whatever it make sense to traverse document references in your model or
if you are breaking document coherency. 

The most useful question that you can ask yourself at that situation is whatever you are looking at the _current_ value (need 
normalization) or the _point in time_ value (use a copy).

And with all of this information now in our head, we can turn to looking at concrete modeling scenarios and how to deal with them.

### Common modeling scenarios 

Giving you advice on how to model your application is beyond the scope of this book. The DDD book is highly recommended in general, but it
isn't always the best choice. Proper DDD modeling takes a lot of discipline and hard work, and it is most appropriate only in specific 
parts of your system (typically the most highly valued ones). RavenDB doesn't have any requirement regarding how you would model your data 
in your application. You can use DDD, you can use business objects or data transfer objects - it doesn't matter to RavenDB. 

What matters is how you _structure_ the data inside RavenDB, and that is what we'll be talking about for the rest of this chapter. We'll 
focus on concrete scenarios, rather than generic modeling advice. Instead of trying to advice you on how to model your entire system, I'm g
going to focus on giving you the tools to build the model as you need it to, so it will play best with RavenDB.

To make things interesting, we are going to use a kindergarden as our data model. As you can expect, we have the notion of children, 
parents, registrations, etc. On the face of it, this is a pretty simple model, we can model it using the code in Listing 3.1.

```{caption="Simple kindergarden model" .cs}
public class Parent
{
	public string Name { get; set; }
}

public class Registration
{
	public DateTime EnrolledAt { get; set; }
	public EnrollmentType Type { get; set; }
}

public class Child
{
	public string Name { get; set; }
	public DateTime Birthday { get; set; }
	public Parent Father { get; set; }
	public Parent Mother { get; set; }
	public Registration Registration { get; set; }
}
```

The code in Listing 3.1 is obviously very simplified model, but it is a good place to start our discussion. A core tenant of modeling in 
RavenDB is that we need to identify what pieces of information belong together, and what pieces are independent of one another. Recall our
discussion on the basics of document modeling design. A good document model has documents that are Independent, Isolated and Coherent. 

With the basic model in Listing 3.1, and our understanding of a kindergarden, we can now proceed to explore aspects of document modeling, 
and we'll start with the most obvious one. In this model, we don't have documents, we _a_ document, just a single one per child. That is 
because we are embedding all information about the child in the document itself. Why would we do that?

#### Embedded documents

A document model is very different from a relational model. It would be very typical in a relational model to have a separate table for 
each class showing in Listing 3.1. A concrete example might work better, and Listing 3.2 shows how this model works for registering Alice
to our kindergarden. 

```{caption="Kindergarden record of Alice in Wunderland" .json}
// children/alice-liddell
{ 	
	"Name": "Alice Liddell",
	"Birthday": "2012-05-04T00:00:00.0000000Z",
	"Mother": {
		"Name": "Lorina Hanna Liddell",
	},
	"Father": {
		"Name": "Henry Liddell"
	},
	"Registration":	{ 
		"EnrolledAt": "2014-11-24T00:00:00.0000000Z",
		"Type": "FullDay"
	}
}
```

All of the information is in one place. You can see that we used a semantic id, `children/` with the child name as the document id. This 
works quite well for data that is well known and predictable. The data itself is centrally located and easy to access. Looking at the data
it is easy to see all relevant infromation about our Alice^[Except maybe which rabbit hole she wondered down to...].

For the most part, embedding infomration is our default approach, because that lead us to more coherent documents, which contain all the 
information relevant to processing it. This is because we aren't limited by format of schema, we can represent arbitrarily complex data
without any issue, and we want to take full advantage of that.

So if the default approach is to embed data, when _wouldn't_ we want to do that? There are a few cases, primarily when we the data doesn't
belong to the same document because it is owned by another entity. A good document model give each document a single reason to change, and
that is the primary force for splitting document apart. 

In the case of the kindergarden record, the obvious example here are the parents. Henry and Lorina are independent entities, and are not 
fully owned by the Alice record. We need to split them into independent documents. On the other side of things, Henry and Lorina had more
children than just Alice, there were also Harry and Edith^[The real Harry and Lorina had a total of 
[10 children](https://en.wikipedia.org/wiki/Alice_Liddell), by the way. ]. So we need to consider how to model such information.

#### Many to one relationship

How do we model a kindergarten where Alice, Harry and Edith are all the children of Henry and Lorina? The technical term for this 
relationaship is `many to one`. Unlike the previous example, where we embedded the parents inside the child document, now we want to model
the data so there is a distinct difference between the different entities. You can see the document model as JSON in Listing 3.3.

```{caption="Many to one modeling with children and parents" .json}
// children/alice-liddell
{
	"Name": "Alice Liddell",
	"MotherId": "parents/1923-A",
	"FatherId": parents/1921-A",
}

// children/henry-liddell
{
	"Name": "Henry Liddell",
	"MotherId": "parents/1923-A",
	"FatherId": parents/1921-A",
}
```

Listing 3.3 shows^[I removed all extra information from the documents to make it clearer.] both Alice and Henry (you can figure out how 
Edith's document looks like on your own) with references to their parents. 
I've intentionally not semantic ids for the parents, to avoid causing confusion about what information is stored on the side holding the
reference. Alice and Henry (and Edith) only hold the _identifier_ for their parents' documents, nothing else.

How does this model reflect in our code? Let's look at Listing 3.4 to see that (again, with some information redacted to make it easier
to focus on the relevant parts).

```
public class Child
{
	public string Name { get; set; }
	public string FatherId { get; set; }
	public string MotherId { get; set; }
}
```

Instead of storing the parent as an embedded document, we just hold the id to that parent. And when we need to traverse from the child
document to the parent document, we do that by following the id. To make things faster, we'll commonly use the `Include` feature to make
sure that we load all those documents in one remote call, but that doesn't impact the data model that we use.

What about the other side, when we need to find all of Lorina's children? Using a query, as shown in Listing 3.5.

```{caption="Loading Lorina and her children" .cs}
using(var session = store.OpenSession())
{
	var lorina = session.Load<Parent>("parents/1923-A");
	var lorinaChildren = (
	 	from c in session.Query<Child>()
		where c.MoethdId == lorina.Id 
		select c
	).ToList();
}
```

As you can see in Listing 3.4 and 3.5, we are being very explicit when we move between documents. RavenDB doesn't allow you to 
transparentely move between different documents, each document is a standalone entity. This help to ensure that you don't create silent
dependencies in your model, since each document is clearly delineated. 

> **Using lazy operations**
>
> In Listing 3.5 you can see that we have to make two separate calls to the server to get all the information we want. When going from
> a child to a parent, we can use the `Include` feature to reduce the number of calls. On the other way, `Include` wouldn't work, but we
> don't have to give up, we have the option of making a few lazy operations and only go to the server once. We'll see exactly how this is
> possible in the next chpater.

The many to one relation is probably the simplest one and it is incredibly common. However, when using it you need to carefully consider
whatever the association should cross a document boundary, or remain inside the document. In the case of parents and children, it is 
obvious that each is a separate entity, but orders and order lines are just the reverse. I the case of orders and order lines it is just
as obvious that order lines do _not_ belong in a separate document but should be part and parcel of the order document. 

There are many cases where that distinction isn't quite so obvious, and you need to give it some thought. The decision can be situational
and is frequently highly dependant on the way you _use_ the data. An equally valid decision in the kindergaden case would be to embed the
parents information in the child document, and just duplicate that infromation for the case where we have two or more siblings in the 
kindergarden a the same time.

It depend what kind of work is done with the parents. If all or nearly all the work is done with the children, then there is no point in 
creating a parent document (it isn't meaningful inside the domain of the kindergarden outside the scope of the children). However, if we 
wanted to keep track of parents as well (for example, Mrs. Liddell takes two sugars in her tea), then we'll likely use a separate 
document.

Children and parents are all well and good, but what about when we push the example up a bit, and explore the world of grandchildren and
grandparents? Now we need to talk about many to many relationships.

#### Many to many relationship

A many to many relationship is a lot more complex than a simple many to one relationship, because they are usually used very differently. 
In our kindergarden example, let us consider how we can model the grandparents. Obviously, each child have multiple grandparents, and each
grandparent can have mutliple children. 

When working with parents and children, it was obvious that we needed to place the association on the children. But how should we model 
grandparents? One way to do it would be to simply model the hierarchy. So a grandparent is the parent of a parent. That seems elegant, 
since this is pretty much what it _is_, but it will lead to a poor model. Grandparents and grandchildren have association between them 
that is completely separated from the one going through the parent, and that deserved to be model on its own, not as a side affect.

The next question is where to put the relationship. We can add a `Grandparents` array to the child document, which will hold all the 
document ids of the grandparents. We can add a `Grandchildren` array to the grandparent document, and store the children ids there. Or
we can do both. What should we choose?

In the context of many to many associations, we always place the record of the assoication on the smaller side. In other words, since a 
child is likely to have fewer grandparents than a grandparent to have children, the assoication is going to be kept on the child 
document. 

> **The users and groups model**
>
> A more technical example that frequently come up within the context of many to many associations is the users & groups model. A user can
> belong to many groups, and a group can have many users. How do we model this? 
>
> A user typically belongs to a few groups, but a group can have a _lot_ of users. So we record the relationship on the smaller side by 
> having a `Groups` property on the user document.

Traversing a many to many association, from the grandchild (smaller) side, can be done by just including and loading the grandparents,
as showing in Listing 3.6.

```{caption="Alice and her grandparents" .cs}
using(var session = store.OpenSession())
{
	var alice = session
		.Include<Child>(c=>c.Grandparents)
		.Load("children/alice-liddell");
	var gradparents = session.Load<Parent>(alice.Grandparents);
}
```

Following the association from the other side requires us to query, as shown in Listing 3.7.

```{caption="Alice and her grandparents" .cs}
using(var session = store.OpenSession())
{
	var grandparent = session.Load<Parent>("parent/1923-A");
	var grandchildren = (
		from c in session.Query<Child>()
		where c.Grandparents.Contain(grandparent.Id)
		select c
	).ToList();
}
```

The code in Listing 3.7 will load Alice's mother and all of her grandchildren. In this case, we can see a slighly more complex query, but 
the essence of it remains the same as the one in Listing 3.5. We have separate documents and clear separation between them. In other 
words, we can query for related data, but we don't just traverse the object graph from one document to another.

#### One to one relationship

A one to one relationship is a pretty strange sight. If there is a one to one relationship, shouldn't this be an embedded document, 
instead of having a separate document? Indeed, in nearly all cases, that would be a better idea.

There are a few reasons why I would want to store a part of a document in a separate document. Usually this is the case if we have a 
document that is conceptually the same, but it has very different access patterns. In the order example, we might have the order header, 
which is very frequently accessed and looked at. And then there is the full order, which might be very big (lots of line items) which we
don't need to access very often. 

In that case, it might make sense to create a document just for the order header (call it `orders/2834/header`). But using a transformer
will be almost as good (we discuss them in [Advanced client API](#advanced-client-api)) and save us the need to split our data. 
The typical way you'll
build one to one relationship in RavenDB is to utilize document id postfixes to create a set of related documents (`orders/2834` and 
`orders/2834/header`, for example).

This tend to result in clearer intent, since it is obvious what each part is doing and what it is meant to be, but even so, that is often
not something that would be advisable. Just putting it all in a single document is easier in most cases.

### Advanced modeling scenarios

The RavenDB modeling techniques that we explored so far are very good in modeling standard business data. You know how to build you 
entities, how to deal with relationship between them (and how to identify whatever they should be separate documents or nested in the same
document). But there is also a lot of sophistication that we can apply to non standard scenarios, which is the topic of this section.

#### Reference data

Reference data is very common, it can be anything from a list of states to the tax rates to localized text to dispaly on the screen. The 
common thread for reference data is that it is typically small and not really interesting in isolation. Such data gets interesting when 
you start working with the lot of them.

It is typical to ignore the modeling concerns for such items, and just throw them in a pile somewhere and not give it any thought. With 
the list of states example, the most natural way to do that is to define a document whose id is `states/ny` and has a single `Name` 
property whose value is obviously `New York`. That would _work_, but it is hardly the best method to go about it. 

Going back to basic principles, such a document will hardly be coherent or independent. Indeed, such a document make very little sense on 
its own. Instead of storing each state and each configuration value as its own document, we'll raise the bar a bit and introduce 
configuration documents. You can see the example in Listing 3.8.

```{caption="Storing the states list as a configuration document" .json}
// config/states
{
    "AL": "Alabama",
    "AK": "Alaska",
    "AS": "American Samoa",
    ...
    ...
    "WV": "West Virginia",
    "WI": "Wisconsin",
    "WY": "Wyoming"
}
```

Modeling reference data in the manner shown in Listing 3.8 has several advantages. It is much easier to work with the data. Instead of 
issuing a query, we can just load the document in one shot, and it is already ready for our consumption. It means that the database has to
do less work, and it plays very nicely into the way RavenDB cache data. It also means that we can reduce deserialization costs, nad make 
it easier to edit and work with the reference data.

In fact, because this is a single document, we can also get RavenDB to do some tasks for us, such as versioning. We'll discuss that in 
more detail in ??XYZ_XYZ??, but RavenDB has builtin versioning capability, which can be _very_ useful when you are talking about reference
or configuration data, since it is easy to look and see what were the changes (or revert them).

#### Hierarchical information

Working with hierarchical data is complex because there are cases where you need to traverse the hierarchy, and that traditionally has 
been expensive. In many cases hierarchies are recursive and often have no limits to the number of levels (although in practice the number
is usually known to be small). 

We need to make a distinction between several types of hierarchies. The first one is quite simple, and can comfortably fit into a single 
document. Typical example of such a hierarchy are comments in a discussion thread. The entire thread is a single document, and all the
comments in the thread always reside in that document. In such a case, storing and working with the hierarchical nature of the data is 
quite trivial, since you'll often just traverse the data directly after loading the document from the server. 

A common example of a hierarchy that doesn't fit the 'everything in a single document' model would be the company hierarchy. Each employee
is an independent entity and we can't store them all as a single document. Instead, we'll strive to model the hierarchy explcitly as an 
independent concept from the employees. In this case, we'll have the notion of a `department`, which will record just the chains of who
reports to whom. The idea is that we separate the hierarchy out since the position of an employee in the hierarchy is orthogonal to most
aspects of the employee. 

The notion of separated hierarchy document gives us a lot of simplicity. Hierarchical operations and queries (all direct and indirect 
reports, for example) are very easy and natural, and it plays nicely with caching and the use of `Include`.

The final example for modeling hierarchies in RavenDB is when we need to model the hierarchy directly in the model. Departments in an 
organization might be a good example, their location in the hierarchy is very important to what they are. In this case, we'll typically
model such a relationship as many to one or many to many relationships at each individual level. That is the simplest method to handle
such a requirement.

This works as long as we need to handle just a single level, but it doesn't handle hierarchial queries. Finding all the departments under
`R&D`, regardless if they are directly or indirectly attached. This requires a bit more work, since we'll need to define indexes that are
able to walk through the hierarchy. We haven't talked about complex operations in indexes, so I'll just mention that RavenDB's indexes can
use `Recurse` to work with hierarchical data and leave that topic to the [Glorious Indexing](#advanced-indexing) chapter, where
we'll cover it in depth.

#### Temporal data model

Temporal data is often a challenge, because it can really mess with the way you think about information. Despite the fancy name, temporal
data is just a way to store data that has a relation to time. The best example I've seen for temporal data is payroll. Consider the notion
of a paycheck. An employer and employee have a contract stipulating that for a given amount of work, a given amount of money will be 
exchanged. 

The problem is that this contract can _change_ over time. An employee can get a raise, have additional vacation days, better overtime 
terms or all of the above. For extra fun, you may get some of those changes retroactively. This sort of thing make it hard to figure out 
exactly what you are supposed to do (consider the case of terms change midmonth, and how you'll handle overtime calculation on a 
shift between 8 PM and 4 AM that fall in that switch). 

The way to model temporal data in RavenDB is to embrace the document nature of RavenDB fully, especially because in most temporal 
domains, the notion of documents and view over time is so important. Consider a paystub that was issued on May 1st, and then a retroactive
pay raise was given. How is that money counted? It is easy to see that when we model the data as physical documents, we don't try to model
a paystub as a mutable entity, but a point in time view. Any changes that were made during the time frame it covered will be reflected in 
the _next_ paystub.

This approach make it much easier on yourself, you don't have to keep track on valid time, effective time and bitemporal time, all at 
once. You just store facts, and the time in which they were stored. Just as if you were keeping all your printed paystubs as a drawer 
somewhere.

The same apply to contracts and other things that mutate over time. Consider the documents seen in Figure 3.2, they represent the same 
contract, with modifications over time as things change.

![Contract documents and changes over time](./Ch03/img02.png)

The key aspect is that when we consider references to this contract, we can select what _kind_ of refence we want. When looking at the 
employee's record, we'll have a referecnce to `contracts/hourly/1234-A`, which is the current version of the contrac, but when issuing
a paystub, we'll always reference a fixed revision of this contract such as `contracts/hourly/1234-A/2013-05-21`. This way, we have set
ourselves up so we choose whatever we want the point in time information or the current (continously updated) version. 

If this sounds similar to the way we decide if we'll copy data from another document to get a point in time snapshot to it or reference it
by id to always get the latest data, that is probably because it _is_ similar. And it simplify dealing with temporal data significantly.

### Additional considerations for document modeling

A major challenge in modeling data with RavenDB is that different features and behavior have very different impact on the cost of various
operations, and that impacts how you'll design and work with your model. In this scetion, we'll explore a few of the features that we 
didn't have a chance to get to know yet, and their impact on modeling data in RavenDB.

I recommend going over this section briefly now and getting back to it when you have finished this book, because by then you'll have the 
complete picture and be able to make more informed decisions. 

#### Handling unbounded document growth 

The size of a document is an interesting question, that frequently come up with regards to modeling. A good range for a document is 
somewhere within the kilobytes range. In other words, if your document is very small (a few dozen bytes), is it really a document? Or 
is it just a single value that would be better off as a reference data document, as we discussed earlier in this chapter. On the other 
hand, if you document is multiple megabytes, how easy it is to work with it?

> **Maximum document size**
> 
> RavenDB have a hard limit on document sizes, around the 2 GB mark (documents are often stored compressed, so a 2GB JSON document
> will typically be smaller inside RavenDB). But the biggest document that I recall seeing in the field was about 170MB. We had a 
> discussion with the customer regarding modeling and about the fact that while RavenDB is _a_ document database, it is perfectly capable 
> of handling _multiple_ documents. 

RavenDB itself, by the way, is just fine with large documents. The problem is everything else. Consider a page that need to dispaly a 12MB
document. What are the costs involved in doing this?

* Reading 12 MB over the network
* Parsing 12 MB of JSON into our entities (this often means using a lot more memory then just the 12 MB of serialized JSON)
* Generating a web page with some or all of the information in the 12 MB document.
* Discard all of this information and let it be garbage collected

In short, there is a _lot_ of work going on, and it can be very expensive. On the other hand, if you have anorexic documents, we'll 
typically need to read many of them to get anything meaningful done, which means a lot of work for the database engine to do.

The rule of thumb we use is that as long as it make sense to measure the document size in kilobytes, you are good. RavenDB will generate 
a warning in the studio when it encounters documents that are too large^[By default, that is set to 5 MB, and is configurable.], but that 
is meant to serve as an early warning system, it has no impact whatsoever on the behavior or performance of the system. 

There are two common reasons why a document can be very large. Either it holds a single (or a few) very large properties (such as a large
text, binary data, etc) or it contains a collection whose size is not bounded.

In the first case, we need to consider whatever the large data is actually a core part of the document, or if it should be stored 
externally. RavenDB supports the notion of attachments, which allow you to, well... attach binary data to a document. If the large data 
can be stored outside the document, regardless of whatever it is binary or textual, that is preferable. Attachments in RavenDB do not have
a size limit, and are not accessed when loading the document, so that is a net win. We'll learn more about attachments toward the end of 
this chapter.

The more complex case if when we have a collection (or collections) inside the document that have no upper bound to them. Consider the 
case of an order document. In retail, an order can contain up to a few hundreds items, but in business to business scenario, it is not
uncommon to have orders that contain tens and hundreds of thousands of items. Putting all of that in a single document is going to lead
to problems, as we already seen.

> **Who modify the big documents**
>
> While big documents are typically frowned upon because of the awkwardness of working with large data (network, parsing and memory costs)
> there is another, more theoretical, aspect to the problems they bring. Who owns a very big document?
>
> Remember that good document modeling implies that there is only a single reason to change a document, but if an order document contains
> so many items, that likely means that there are multiple sources that add items to the order (such as different departments working with
> the same supplier). At that point, it isn't a single document that we are working with, it is multiple independent orders that are 
> merged into a single final shipping / billing order. 
>
> Again, falling back to the real world example, we would find it very strange if a van stopped by our store and started unpacking crates
> full of pages detailing all the things that a customer wanted to purchase, but when we are working on a single document, this is pretty
> much what we are doing. Instead, we'll typically accept indiviual requests from each contact person / department, and associate them 
> with the appropriate billing / shipping cycle. 
> 
> In other words, the _reason_ we split the document isn't so much to artificially reduce its size, but because that is how the business
> will typically work, and doing it any other way is _really_ hard. Splitting it so the document structure follow the structure of the 
> business is usually the right thing to do.

So how do we handle large documents? Typically by splitting them into smaller pieces along some sort of business boundary. Let us consider
the case of a bank account and the transactions that we need to track. Keeping all the transactions inside the account document is not 
feasible on a long term basis. But how are we going to split them? Here we look at the business and see that the bank itself doesn't look
at the account and all its history as a single unit.

Instead, the bank talk about transactions that happened in a particular business day or a particular month. And we can model our documents
accordingly. In the bank example, we'll end up with the following documents:

* `accounts/1234` - account details, status, ownership, conditions, etc.
* `accounts/1234/txs/2017-05` - the transactions in this account on May 2017
* `accounts/1234/txs/2017-05` - the transactions in this account on April 2017

Within a given time frame, we can be reasonably certain that there is going to be an upper bound to the number of transactions in an 
account. I quite like this approach, because it aligns very closely with how the business is thinking about the domain and it result in
very clear separation of documents. You can't always split such things apart on such a convienent boundary, what happen if there isn't a
time element that you can use?

At that point, you still split the document, but you do so arbitrarily. You can decide that you'll generate a new document for the items
in the collection every `N` items, for example. Typically that `N` value is some multiple of the typical page size that you have in your
system. If we'll go back to the example of order and items in the order, and assuming we don't go with the route of splitting the order
itself into separate requests, we'll just split the order every 100 items, so we'll end up with the following documents:

* `orders/1234` - the order header, shipping details, billing, etc.
* `orders/1234/items/1` - the first 100 items for this order.
* `orders/1234/items/2` - the second 100 items for this order.

The order document will contain the ids of all the items documents (so we can easily `Include` them) as well as the id of the last items
document, so we'll know where to add an item to this order.

This approach is a bit more clumsy in my eyes, but if we don't have another alternative natural boundary, this will work fine.

#### Cached queries properties

A common modeling technique is keep track of some cached global state inside our entities. The most trivial example I can think about is 
the `customer.NumberOfOrders` property. I want to call out this behavior specifically because it is common, it make sense and it is 
usually a bad idea.

But why? Well, let us consider why we need to have this property in the first place. In most domains, a property such as `NumberOfOrders` 
does not comprise an intrisinc part of the customer. In other words, that information is interesting, but it doesn't _belong_ inside the 
customer document. Moreover, in order to keep this property up to date, we need to update it whenever we add an order, decrement it when
an order is deleted, etc. For that matter, what do we do about an order that was returned? Does it count, what about the replacement 
order? 

In other words, this is a pretty hefty business decision, but most of the the time, this property is added because we want to show that
value in the user interface, and issuing an aggregation query can be expensive. Or, rather, an aggregation query is expensive if you 
aren't using RavenDB.

With RavenDB, aggregation is handled via Map/Reduce operations, and you don't pay any cost at all for querying on the results of the 
aggregation. We dedicate a whole chapter ([Map/Reduce](#map-reduce)) to discussing this topic, so I'll go into the details here, but the impact
of such a feature is profound. Cheap aggregation means that you can do a lot more aggregation, and you don't have to manage it yourself.

That frees you from a lot of worries, and means that you can focus on building your model as it is used, letting RavenDB handle the side
channel operations, like calculating exactly how many orders a customer has, and in what state.

Another problem with `customer.NumberOfOrders` is how you update it. Two orders being generated at once for the same customer may result
in lost updates and invalid data. How does RavenDB handle concurrency, and how does it impact your model?

#### Concurrency control

RavenDB is inherently concurrent, capable of handling hundreds of thousands^[Note a typo, in our benchmarks, we are always saturating the
network long before we saturate any other resource, so the current limit is how fast your network card(s) are in packet processing.] 
requests per second.

That lead to an set of interesting problems regarding concurrency. What happens if two requests are trying to modify the same document at
the same time? That depends on exactly what you asked RavenDB to do. If you didn't do anything, RavenDB will execute those two 
modifications one at a time, and the last one would win (there is no way to control which that would be). Note that _both_ operations 
will execute, and behaviors such as versioning will apply to both operations.

> **ETags in RavenDB**
> 
> RavenDB make extensive use of the notion of etags. An etag is a 64 bits number that is incremented on every operation in RavenDB. Each
> time a document is modified is it associated with the current etag value. The etag is used for optimisitic concurrency control, for
> various internal operations and for caching. 
> 
> Etags are local for each node, and isn't globally shared (we'll learn more about etags and change vectors in 
> [Your first RavenDB Cluster](#clustering-intro)). 

The `last write wins` model is the default option in RavenDB, except when you are creating a new document. When the RavenDB Client API
is generating the id, it _knows_ that it intend to create a new document, and it sets the expected etag accordingly, so if the document
already exist, and error will be generated.

What about when you want to take advantage of this feature? You can ask RavenDB to enable optimistic concurrency at several levels. The 
following code enables optimistic concurrency to all sessions created by this document store.

	store.Conventions.UseOptimisticConcurrency = true;

And you can also enable this on a more selective basis, on particular session, using the following snippet:

	session.Advanced.UseOptimisticConcurrency = true;

Or on a single particular operation:

	session.Store(entity, etag);

In either case, the RavenDB Client API will send the expected etag to the server, which will compare it to the current etag. If the 
current etag does not match the expected etag, a `ConcurrencyException` will be thrown, aborting the entire transaction. 

The first two options uses the etag that the server supllied when the document was loaded, in other words, they would error if the 
document was modified between the time we fetched it from the server and the time we wrote it back. The third option is a lot more
interesting, though. 

The ability to specify the etag on a specific entity is quite important, because that allow you to perform offline optimistic concurrency
checks. You can render a web page to the user, and along the document data there, include the etag that you got when you loaded that 
document. The user can look at the document, modify it, etc. And then send it back to your application. At that point, you'll store the 
document with the tag as it was when we rendered the page to the user, and then call `SaveChanges`. If the document has been modified by
anyone in the meantime, the operation will fail.

In this manner, you don't need to keep the session around, you just need to pass the etag back and forth. As you can see, the etag is 
quite important for concurrency, but it plays another important role with caching.

#### Caching

RavenDB uses `REST` over `HTTP` for communcation between client and server. That means that whenever you load a document, or perform a
query, you are actually sending an `HTTP` call over the wire. This let RavenDB take advantage on the nature of `HTTP` to get play some 
nice tricks. 

In particular, the RavenDB Client API is capable of using the nature of `HTTP` to enable transparent caching of all requests from the 
server. The Client API does that with some assistance from the server. Here is how it works, each response from the server contains an 
`Etag` header. In the case of loading a single document, that `ETag` header is also the document etag, but if we are querying or loading
multiple documents that `ETag` header will contain a computed value. 

On the client side, we'll cache the request from the server alongside the URL and the `ETag` value. When a new request comes in for the 
same URL, we'll send it to server, but we'll also let the server know that we have the result of the request with a specific `ETag` value.
On the server side, there are dedicated code paths that can cheaply check if the `ETag` that we have on the client is the same as the 
current one, and if it is, we'll just let the client know that it can use the cached version and that is all.

We don't have to execute any further code, or to send anything other than `304 Not Modified` to the client. And on the client side, there 
is no need to download any data from the server, we can access our local copy, already parsed and primed for us. That can represent a 
signficant speed saving in some cases, because we have to do a lot less work.

If the data has changed, we'll have a different `ETag` for the request, and the server will process it normally. The client will replace
the cached value and the process will repeat. You can take advantage of that when designing your software, because you know that certain
operations are very likely to be cached and thus cheap. 

RavenDB also have a feature called `Aggressive Caching`, which allow the RavenDB Client API to register for changes from the server. At 
that point, we don't even need to make a call to the server if we already have a cached value. We can wait for the server to let us know 
that something have changed and that we need to call back to the database to see what changed. For certain types of data (config / 
reference documents in particular, that can be a major performance saving).

#### Reference handling, Include and LoadDocument

Earlier in this chapter we looked at various types of relationships between objects and documents. From embedded a value inside a larger
document to many to one, many to many, etc. While we were focused on building our model, we skipped a bit on explain how we work with such
relationship beyond the diagram stage.

There are two separate operations that we need to take into account. Fetching related information during a query or a document load can be
done using an `Include` call, which we already covered in [Zero to RavenDB](#zero-to-ravendb). I'm pointing this out again because this is a 
very important feature, which deserved to be called out (and used) a lot more. It is incredibly powerful tool to reduce the number of 
server requests when working with complex data.

The second operation is querying, more to the point, when I want to query for a particular document based on the properties of a related
document. Going back to the kindergarden example, searching for all the children whose mother's name is Lorina would be a great example. 
The child document no longer contain the name of the parent, so how are we going to search for that? RavenDB doesn't allow you to perform
joins, but it does allow you to index related data during the indexing phase, and then query on that. So a querying the children by their
mother's name is quite easy to do, using the `LoadDocument` feature. `LoadDocument` is discussed in full in 
[Indexing the path to glory](#advanced-indexing).

I'm mentioning it here because knowing that it exists has an impact on the way we model data. Be sure to read all about what 
`LoadDocument` can do and how it works, it can be of great help when deciding how to model related data. In the end, though, the decision
almost always fall down on the issue of whatever we want a point in time view of the data or the current value. For the first option, 
we'll typically copy the value from the source to its own document, and for the second, we can use `LoadDocument` during indexing and 
querying and `Include` when fetching the data from the server.

#### Attachments and binary data

RavenDB is a JSON document database, but not all data can be (or should be) represented in JSON. This is especially true when you consider 
that a document might very well be composed of additional datum. For example, when working with an order document, then the invoice PDF might
be something taht we need to keep^[A signed PDF invoice is pretty common, and you are required to keep it for tax purposes, and cannot just
generate it on the fly]. If we have a user document, then the profile picture is another piece of binary data that is both part of the 
document and separate from it.

RavenDB's answer to that need is attachments. Attachments are binary data that can be attached to documents. An attachment is always on a
document, and beyond the binary data, and attachment a name (which must be unique within the scope of the parent document). Attachments are
stored on the same storage as the documents (althouh in a separate location) and can be worked with together with documents.

That means that attachments can participate in the same transaction as documents with the same `ACID` semantics (all in or none at all). 
There is no size limit to the size of attachments, and a single document can have multiple attachments. 

The ability to store binary data easily is very important for our modeling, especially because we can treat attachment changes as part of
our transaction. That means that operation such as "the leased is signed" can be done in a single transaction that includes both the update
of the `Lease` document and the storage of the signed lease scan in the same operation. That save you a bit of a headache with error 
handling.

When modeling, consider whatever external data is strongly related to the document and should be stored as an attachment. The easiest mental
model I have for doing this is to consider attachments in email. Imagine that the document is the email content, and the attachments are
just like attachments in email. Typically such attachment provide additional information about the topic in question, and that is a good
usecase for attachments in RavenDB.

#### Versioning and Auditing

Tracking changes in data over time is often a challenging task. Depending on the domain in quesion^[This is very common in Legal and 
Healthcare, for example] we need to be able to show all changes that happened to a document. RavenDB support that using the versioning
feature.

The database administrator can configure RavenDB to keep track of document revisions. Whenever a document is changed, an immutable 
revision will be created, which can be used later on to follow all changes that happened to a document. RavenDB can be set to track
only specific collections and only keep track of the last N revisions, but it is very common to just say "track everything", since
disk space is relatively cheap and in those domain that need this accountability, it is better to keep too much than not enough.

Revisions also can apply to deletes, so you can restore a deleted document if you need to. One way of looking at versioning is as a way
to have a backup of all changes on a per document level^[Obviously that does not alleviate the need to have proper backups.]. 

Auditing is a _bit_ different. While revisions tell you _what_ changed, auditing tell you by _whom_ and typically what _for_. RavenDB
support this kind of auditing using client side listeners, which can provide additional context for the document whenever it is changed.
We'll discuss listeners and their use in more dpeth in [Advanced client API](#advanced-client-api).

#### Expiration

When storing documents inside RavenDB, you can specify that they will expire at a given point in time. RavenDB will periodically remove
all the expired documents automatically. This is a relatively small feature, but it make it very nice to implmenet features such as
"the link in this email will expire in 12 hours". 

We'll see how to use the expiration feature in [Advanced client API](#advanced-client-api). There isn't much to say about the expiration feature beside that
it works, but it should be noted that it is a soft expiration. In other words, if you speciifed an expiration time, the document might 
still be there after that time has passed, because RavenDB didn't get around to clean it yet. By default, RavenDB will clear expired
documents every minute, so the document might live just a bit longer than expected, but not terribly so.

### ACID vs. BASE

The final topic in the modeling chapter is one of the more important ones. `ACID` stands for Atomic, Consistent, Isolated and Durable while 
`BASE` `stands for Basically Available, Soft state, Eventually consistent. Those are two very different approach for approaching how we
deal with data.

RavenDB uses _both_, in different parts of its operations. In general `ACID` is what we always strive for. A fully consistent model make
it easy to build upon and reason about, but it also can be quite expensive to build and maintain. In a distributed system, ensuring atomicity
can be _quite_ expensive, since you need to talk to multiple machines for each operation, for example. On the other hand `BASE` give us a lot more freedom, and that translate into a lot more optimization opportunities. 

In RavenDB, all operations that operate on a document or attachment using their id (put, modify, delete) are always consistent and run in an
`ACID` transaction. Bulk operations over sets of documents (update all that match a particular query, bulk insert, etc) are composed of 
multiple separate transactions instead of one very big one. 

By default, when you save a document into RavenDB, we'll aknowledgge the write when the data has been saved on one node in a durable 
fashion^[That means that the data has been written to disk and fsync() or equivalent was called, so the data is safe from power loss].
You can also ask the write to be acknowledged only when the write has been made durable on multiple nodes (using 
the `WaitForReplicationAfterSaveChanges` method) for additional safety. We discuss  all such details in 
[Your first RavenDB Cluster](#clustering-intro).

One of the typical balancing acts that database administrators have to do is to choose how many indexes they will have. Too many indexes
and the write process grinds to a halt. On the other hand, not enough indexes and your queries are going to require table scans, which are
_horrendously_ expensive even on moderate sized database.

The reason for the tradeoff is that a transaction must update all the relevant indexes on every change. That means that index updates are 
right there in the main pathway for updating data, which explain why they can so severely degrade performance. When desiging our indexing
implmenetation, we took a different approach.

Indexing in RavenDB are handled as an asynchronous task running in the background whenever there are updated to the database. This means that
we don't have to wait for the index to finish updating before completeing the write. And that means that we have far better opportunities
for optimizations. For example, we can roll up multiple separate changes that happened in different transactions into the same index update
batch.

This also allow us to be able to prioritize certain operations on the fly. If we are under a load of pressure right now, we can reduce the 
amount of time we spend indexing in order to serve more requests. This follows the idea that we always want to be able to return a result 
from RavenDB as soon as possible, and that in many cases a triple checked answer that came too late is worst than useless.

Indexes in RavenDB are `BASE`, in the sense that they can lag behind the documents they reflect. In practice, indexes are kept up to date and
the lag time between a document update and index update is typically measured in microseconds. The `BASE` nature of indexes allows us to 
achieve a number of desirable properties. Adding an index to the system doesn't block anything, and updating an index definition can be done in
a side by side manner. Various optimizations are possible because of this (mostly related to batching and avoidance of locks).

> **What are the costs in waiting**
>
> Waiting for indexing or waiting for replication is something that you have to explcitly ask for, because it costs, but what _are_ those
> costs? In the case of replication, that means waiting for another server (or servers) to get the information and also persist it safely
> to disk. That can add latency in the order of tens of milliseconds when running on a local data center, and hundreds of milliseconds if
> your cluster is going over the public internet.
>
> For indexing, this is usually a very short amount of time, since the indexes are usually updated very quickly. If there is a new index or
> it may take longer, because the write will have to wait for the index to catch up not just with out write, but with all the writes that 
> the new index is processing.
>
> We typically measure RavenDB performance in tens or hundreds of thousands of requests per second. Even going with just 10K request / sec
> that give us a budget of 1/10 of a millsecond to process a request. Inside RavenDB requests are handled in an async manner, so it isn't as
> if we are holding hundreds of threads open, but additional additional waits for the request is something we want to avoid.
>
> For the most part, those costs are pretty small, but there are times where they might be much higher. If there is a new index running, and
> we asked to wait for all indexes, we may wait for a while. And we want the default behavior to be as predictable as possible. That isn't
> meant to discourage you from using those features, but it does mean that we suggest that you'll give it some thought.
>
> In particular, spraying the waits at any call is possible and it will work, but it usually not the best solution. You typically want to apply
> them for specific scenarios, either high value writes (and you want to make sure that it is saved on multipel nodes) or if you intend to issue
> a query immediately after the write and want to ensure that it will be included in the results. Otherwise, you should accept the defaults and
> allow RavenDB the freedom to optimize its operations.

The `BASE` on indexign behavior is optional, you can ask RavenDB to wait for the indexes to update (using the `WaitForIndexesAfterSaveChanges` method) 
so you have the option of choosing. If this is a write that you intend to immediately query, you can force a wait, but usually that isn't require, so
you don't need to pay the cost here. 

You need to be aware of the distinctions here between queries (which can be `BASE` and lag a bit^[The typical lag time for indexing is under 1ms]) and 
bulk operations (many small transactions) and operations on specific documents (either one at a time or in groups) which happen as `ACID` transaction.
Taking advantage of the `BASE` nature of indexing allow you to reduce the cost of querying and writing and allow you to selectively apply the decision
on a case by case basis.

### Summary

Modeling data in RavenDB isn't a trivial topic, and it isn't over just because you finished this chapter. I tried to highlight certain features and their 
impact of modeling, but I suggest finishing the book and then reading this chapter again, with fresh perspective and better understanding of how things are
playing together.

We started by looking at RavenDB as a simple key/value store. Just throw data in (such as session information, shopping cat, etc) and access it by well known
key. That has the advantage of giving us very fast operations, but without hiding the data behind an opaque barrier. We can also use the expiration feature
to store time dependent data. 

From the trivial model of key/value store, we moved to considering real document models. Looking at how documents in the real world, phyiscal ones, are 
modeled can be very helpful there. Especially since this is usually much easier to explain to the business. Reducing the impedenace mismatch beteween the
business and the application is always a good thing, and proper modeling can help there. Of course, don't go to far, remember that documents are virtual 
concepts and aren't limited to the phyiscal world. We then looked at the the issue of denormalization and noted the difference between the same _value_ of
the data and the same _meaning_ of the data. 

The next topic was actual modeling, we looked at how we should structure embedded values to store data that is intrinsically part of the document, instead
of holding that data outside of the document. We looked at how to model relations and collections, many to one and many to many associations. Then we covered
more advanced modeling techniques, such as how to handle reference and configuration data, how to deal with temporal infromation and deal with hierarchical
structures.

Next we went over some of the constraints that we have to take into account when modeling. How to deal with the growth of document and what constitute a good
size for a document in RavenDB. We looked at concurrency control and etags are useful for optimistic concurrency and for caching and why we should avoid caching
aggregated data in our model (`NumberOfOrders`).

We looked at handling binary data with attachments, auditing and change tracking using the versioning feature and learned that we can expire documents natively
in RavenDB. Reference handling at indexing and query time was briefly covered (we'll cover it in depth in the [Glorious Indexing](#advanced-indexing)) as it is important
for how you model your documents.

A repeating theme is the the use of semantic ids to give you better control over documents and their meaning. Not so much for RavenDB's sake^[The database 
doesn't care what your document ids look like.] but because it increase understanding and visibility in your domain. Using document ids to "nest" documents
such as `accounts/1234/tx/2017-05` or having meanigful document ids such as `config/states` helps a lot in setting out the structure for your model.

The final topic we covered in this topic is `ACID` vs. `BASE` in RavenDB. Documents in RavenDB are stored and accessed in an `ACID` manner, while we default
to `BASE` queries to get higher performance and have more chances for optimizations. This behavior is controlled by the user on a case by case basis, so you
can select the appropriate mode for each scenario.

Our next chapter is going to cover the client API in depth, going over all the tools that you have to create some really awesome behaviors. After that, we'll
get to running clusters of RavenDB and how the distributed portion of RavenDB is handled. 
